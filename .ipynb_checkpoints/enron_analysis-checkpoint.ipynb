{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c4487d",
   "metadata": {},
   "source": [
    "# Enron Email Network Analysis\n",
    "\n",
    "Builds a directed communication network from the Enron email corpus and exports a Gephi-compatible `.gexf` file with useful node metrics and degree-driven pruning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e03088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from email.utils import getaddresses, parsedate_to_datetime\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- CONFIG -----\n",
    "INPUT_MODE = 'dir'  # 'dir' for raw maildir structure, 'csv' for pre-flattened table\n",
    "INPUT_PATH = '/path/to/your/enron/maildir'  # update with the local path\n",
    "CSV_FROM_COL = 'From'\n",
    "CSV_TO_COLS = ['To', 'Cc', 'Bcc']\n",
    "CSV_DATE_COL = 'Date'  # set to None when the column is unavailable\n",
    "\n",
    "# Pruning thresholds\n",
    "MIN_EDGE_WEIGHT = 2  # drop edges with weight < this\n",
    "KEEP_GIANT_COMPONENT = True\n",
    "TOP_N_NODES = None  # fallback: keep top N nodes by degree when auto pruning is disabled\n",
    "\n",
    "# Degree-driven pruning\n",
    "AUTO_PRUNE_BY_DEGREE = True\n",
    "PRUNE_DEGREE_MIN = None        # e.g., 5 to require at least 5 total connections\n",
    "PRUNE_DEGREE_QUANTILE = 0.9    # e.g., 0.9 keeps top 10% by degree; set to None to skip\n",
    "PRUNE_DEGREE_TOP_N = None      # e.g., 1000 keeps the top N nodes by degree if others unset\n",
    "\n",
    "# Optional time slicing (use 'M' for monthly, 'Q' for quarterly, or None)\n",
    "TIME_SLICE_FREQ = None\n",
    "DATE_MIN = None  # e.g., '2000-01-01'\n",
    "DATE_MAX = None  # e.g., '2002-12-31'\n",
    "\n",
    "OUTPUT_DIR = './gephi_exports'\n",
    "BASE_GEXF_NAME = 'enron_network.gexf'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "EMAIL_RE = re.compile(r'<?([^<>@,\\s]+@[^<>,\\s]+)>?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c77f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_addresses(raw_list):\n",
    "    addrs = []\n",
    "    for _, addr in getaddresses(raw_list):\n",
    "        addr = (addr or '').strip().lower()\n",
    "        match = EMAIL_RE.search(addr)\n",
    "        if match:\n",
    "            addr = match.group(1)\n",
    "        if addr:\n",
    "            addrs.append(addr)\n",
    "    return addrs\n",
    "\n",
    "\n",
    "def parse_date_safe(val):\n",
    "    if not val:\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(val)\n",
    "        return pd.to_datetime(dt)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(val, errors='coerce')\n",
    "\n",
    "\n",
    "def iter_emails_from_dir(root_path):\n",
    "    root = Path(root_path)\n",
    "    files = [p for p in root.rglob('*') if p.is_file()]\n",
    "    for path in tqdm(files, desc='Reading emails from dir'):\n",
    "        try:\n",
    "            with open(path, 'rb') as handle:\n",
    "                message = BytesParser(policy=policy.default).parse(handle)\n",
    "        except Exception:\n",
    "            continue\n",
    "        sender = normalize_addresses([message.get('From', '')])\n",
    "        to_list = normalize_addresses([message.get('To', '')])\n",
    "        cc_list = normalize_addresses([message.get('Cc', '')])\n",
    "        bcc_list = normalize_addresses([message.get('Bcc', '')])\n",
    "        date_val = parse_date_safe(message.get('Date', None))\n",
    "        yield {\n",
    "            'from': sender[0] if sender else None,\n",
    "            'to': to_list,\n",
    "            'cc': cc_list,\n",
    "            'bcc': bcc_list,\n",
    "            'date': date_val,\n",
    "        }\n",
    "\n",
    "\n",
    "def iter_emails_from_csv(csv_path, from_col, to_cols, date_col=None):\n",
    "    df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    dates = pd.Series(pd.NaT, index=df.index)\n",
    "    if date_col and date_col in df.columns:\n",
    "        dates = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc='Reading emails from csv'):\n",
    "        sender = normalize_addresses([row.get(from_col, '')])\n",
    "        recipients_raw = [row.get(col, '') for col in to_cols if col in df.columns]\n",
    "        recipients_split = []\n",
    "        for raw in recipients_raw:\n",
    "            if raw:\n",
    "                recipients_split.extend(re.split(r'[;,]', raw))\n",
    "        recipients = normalize_addresses(recipients_split)\n",
    "        yield {\n",
    "            'from': sender[0] if sender else None,\n",
    "            'to': recipients,\n",
    "            'cc': [],\n",
    "            'bcc': [],\n",
    "            'date': dates.iloc[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def build_edges(email_iter):\n",
    "    records = []\n",
    "    for message in email_iter:\n",
    "        sender = message['from']\n",
    "        if not sender:\n",
    "            continue\n",
    "        recipients = list(set((message['to'] or []) + (message['cc'] or []) + (message['bcc'] or [])))\n",
    "        for recipient in recipients:\n",
    "            if recipient and recipient != sender:\n",
    "                records.append((sender, recipient, message['date']))\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=['source', 'target', 'date', 'weight'])\n",
    "    edge_df = pd.DataFrame(records, columns=['source', 'target', 'date'])\n",
    "    edge_df['weight'] = 1\n",
    "    return edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3ee2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_date(df_edges, date_min=None, date_max=None):\n",
    "    if 'date' not in df_edges.columns:\n",
    "        return df_edges\n",
    "    filtered = df_edges.copy()\n",
    "    if date_min:\n",
    "        filtered = filtered[(filtered['date'].isna()) | (filtered['date'] >= pd.to_datetime(date_min))]\n",
    "    if date_max:\n",
    "        filtered = filtered[(filtered['date'].isna()) | (filtered['date'] <= pd.to_datetime(date_max))]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def aggregate_edges(df_edges):\n",
    "    return df_edges.groupby(['source', 'target'], as_index=False).agg(weight=('weight', 'sum'))\n",
    "\n",
    "\n",
    "def compute_node_metrics(df_edges_agg):\n",
    "    if df_edges_agg.empty:\n",
    "        return pd.DataFrame(columns=['Id', 'out_weight', 'in_weight', 'out_degree', 'in_degree', 'degree', 'weighted_degree', 'Label'])\n",
    "    out_weight = df_edges_agg.groupby('source')['weight'].sum().rename('out_weight')\n",
    "    in_weight = df_edges_agg.groupby('target')['weight'].sum().rename('in_weight')\n",
    "    out_degree = df_edges_agg.groupby('source').size().rename('out_degree')\n",
    "    in_degree = df_edges_agg.groupby('target').size().rename('in_degree')\n",
    "    nodes = pd.Index(\n",
    "        out_weight.index.tolist() + in_weight.index.tolist() + out_degree.index.tolist() + in_degree.index.tolist()\n",
    "    ).unique()\n",
    "    nodes_df = pd.DataFrame({'Id': nodes})\n",
    "    nodes_df = (\n",
    "        nodes_df\n",
    "        .merge(out_weight, left_on='Id', right_index=True, how='left')\n",
    "        .merge(in_weight, left_on='Id', right_index=True, how='left')\n",
    "        .merge(out_degree, left_on='Id', right_index=True, how='left')\n",
    "        .merge(in_degree, left_on='Id', right_index=True, how='left')\n",
    "    )\n",
    "    for col in ['out_weight', 'in_weight', 'out_degree', 'in_degree']:\n",
    "        nodes_df[col] = nodes_df[col].fillna(0).astype(int)\n",
    "    nodes_df['degree'] = nodes_df['out_degree'] + nodes_df['in_degree']\n",
    "    nodes_df['weighted_degree'] = nodes_df['out_weight'] + nodes_df['in_weight']\n",
    "    nodes_df['Label'] = nodes_df['Id']\n",
    "    return nodes_df\n",
    "\n",
    "\n",
    "def keep_giant_component(df_edges_agg):\n",
    "    graph = {}\n",
    "    for _, row in df_edges_agg.iterrows():\n",
    "        graph.setdefault(row['source'], set()).add(row['target'])\n",
    "        graph.setdefault(row['target'], set()).add(row['source'])\n",
    "    visited = set()\n",
    "    components = []\n",
    "    for node in graph:\n",
    "        if node in visited:\n",
    "            continue\n",
    "        stack = [node]\n",
    "        component = set()\n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            component.add(current)\n",
    "            stack.extend([nbr for nbr in graph.get(current, []) if nbr not in visited])\n",
    "        components.append(component)\n",
    "    if not components:\n",
    "        return df_edges_agg\n",
    "    giant = max(components, key=len)\n",
    "    mask = df_edges_agg['source'].isin(giant) & df_edges_agg['target'].isin(giant)\n",
    "    return df_edges_agg[mask]\n",
    "\n",
    "\n",
    "def keep_top_n_nodes(df_edges_agg, n):\n",
    "    top_nodes = compute_node_metrics(df_edges_agg).sort_values('degree', ascending=False)['Id'].head(n)\n",
    "    node_set = set(top_nodes)\n",
    "    return df_edges_agg[df_edges_agg['source'].isin(node_set) & df_edges_agg['target'].isin(node_set)]\n",
    "\n",
    "\n",
    "def prune_by_degree(df_edges_agg, min_degree=None, quantile=None, top_n=None):\n",
    "    metrics_full = compute_node_metrics(df_edges_agg)\n",
    "    if metrics_full.empty:\n",
    "        return df_edges_agg, metrics_full, None, 0\n",
    "\n",
    "    threshold = None\n",
    "    if min_degree is not None:\n",
    "        threshold = float(min_degree)\n",
    "        nodes_to_keep = metrics_full[metrics_full['degree'] >= min_degree]['Id']\n",
    "    elif quantile is not None:\n",
    "        threshold = float(metrics_full['degree'].quantile(quantile))\n",
    "        nodes_to_keep = metrics_full[metrics_full['degree'] >= threshold]['Id']\n",
    "    elif top_n is not None:\n",
    "        sorted_metrics = metrics_full.sort_values('degree', ascending=False)\n",
    "        nodes_to_keep = sorted_metrics['Id'].head(top_n)\n",
    "        if len(sorted_metrics) >= 1:\n",
    "            threshold = float(sorted_metrics['degree'].iloc[min(top_n - 1, len(sorted_metrics) - 1)])\n",
    "    else:\n",
    "        return df_edges_agg, metrics_full, None, len(metrics_full)\n",
    "\n",
    "    nodes_set = set(nodes_to_keep)\n",
    "    pruned_edges = df_edges_agg[\n",
    "        df_edges_agg['source'].isin(nodes_set) & df_edges_agg['target'].isin(nodes_set)\n",
    "    ]\n",
    "    pruned_metrics = compute_node_metrics(pruned_edges)\n",
    "    return pruned_edges, pruned_metrics, threshold, len(nodes_set)\n",
    "\n",
    "\n",
    "def export_gexf(df_edges_agg, out_path, node_metrics=None):\n",
    "    if node_metrics is None:\n",
    "        node_metrics = compute_node_metrics(df_edges_agg)\n",
    "    graph = nx.DiGraph()\n",
    "    for _, row in df_edges_agg.iterrows():\n",
    "        graph.add_edge(row['source'], row['target'], weight=int(row['weight']))\n",
    "    for _, row in node_metrics.iterrows():\n",
    "        graph.add_node(row['Id'])\n",
    "        graph.nodes[row['Id']].update({\n",
    "            'label': row['Label'],\n",
    "            'out_degree': int(row['out_degree']),\n",
    "            'in_degree': int(row['in_degree']),\n",
    "            'degree': int(row['degree']),\n",
    "            'out_weight': int(row['out_weight']),\n",
    "            'in_weight': int(row['in_weight']),\n",
    "            'weighted_degree': int(row['weighted_degree']),\n",
    "        })\n",
    "    nx.write_gexf(graph, out_path, encoding='utf-8')\n",
    "    return out_path, node_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c569882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading emails from dir: 100%|██████████| 517404/517404 [08:17<00:00, 1040.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw edges: 3610073\n",
      "Unique directed edges: 361032\n",
      "After weight >= 2: 269781 edges\n",
      "After giant component: 269133 edges\n",
      "After degree-based pruning (threshold ~= 13.00, nodes kept = 6275): 166160 edges\n",
      "GEXF exported to: gephi_exports/enron_network.gexf\n"
     ]
    }
   ],
   "source": [
    "if INPUT_MODE == 'dir':\n",
    "    email_iterable = iter_emails_from_dir(INPUT_PATH)\n",
    "elif INPUT_MODE == 'csv':\n",
    "    email_iterable = iter_emails_from_csv(INPUT_PATH, CSV_FROM_COL, CSV_TO_COLS, CSV_DATE_COL)\n",
    "else:\n",
    "    raise ValueError(\"INPUT_MODE must be 'dir' or 'csv'\")\n",
    "\n",
    "edges_df = build_edges(email_iterable)\n",
    "print(f'Raw edges: {len(edges_df)}')\n",
    "\n",
    "edges_df = filter_by_date(edges_df, DATE_MIN, DATE_MAX)\n",
    "edges_agg = aggregate_edges(edges_df)\n",
    "print(f'Unique directed edges: {len(edges_agg)}')\n",
    "\n",
    "if MIN_EDGE_WEIGHT and MIN_EDGE_WEIGHT > 1:\n",
    "    edges_agg = edges_agg[edges_agg['weight'] >= MIN_EDGE_WEIGHT]\n",
    "    print(f'After weight >= {MIN_EDGE_WEIGHT}: {len(edges_agg)} edges')\n",
    "\n",
    "if KEEP_GIANT_COMPONENT:\n",
    "    edges_agg = keep_giant_component(edges_agg)\n",
    "    print(f'After giant component: {len(edges_agg)} edges')\n",
    "\n",
    "node_metrics = None\n",
    "if AUTO_PRUNE_BY_DEGREE:\n",
    "    edges_agg, node_metrics, degree_threshold, kept_nodes = prune_by_degree(\n",
    "        edges_agg,\n",
    "        min_degree=PRUNE_DEGREE_MIN,\n",
    "        quantile=PRUNE_DEGREE_QUANTILE,\n",
    "        top_n=PRUNE_DEGREE_TOP_N,\n",
    "    )\n",
    "    if degree_threshold is not None:\n",
    "        print(f'After degree-based pruning (threshold ~= {degree_threshold:.2f}, nodes kept = {kept_nodes}): {len(edges_agg)} edges')\n",
    "    else:\n",
    "        print(f'Degree pruning skipped (configuration yielded no threshold). Nodes kept = {kept_nodes}.')\n",
    "elif TOP_N_NODES:\n",
    "    edges_agg = keep_top_n_nodes(edges_agg, TOP_N_NODES)\n",
    "    print(f'After top {TOP_N_NODES} nodes: {len(edges_agg)} edges')\n",
    "\n",
    "if node_metrics is None:\n",
    "    node_metrics = compute_node_metrics(edges_agg)\n",
    "\n",
    "main_gexf_path = Path(OUTPUT_DIR) / BASE_GEXF_NAME\n",
    "export_gexf(edges_agg, main_gexf_path, node_metrics=node_metrics)\n",
    "print(f'GEXF exported to: {main_gexf_path}')\n",
    "\n",
    "if TIME_SLICE_FREQ:\n",
    "    if 'date' not in edges_df.columns or edges_df['date'].isna().all():\n",
    "        print('No usable dates available for time slicing.')\n",
    "    else:\n",
    "        edges_with_dates = edges_df.dropna(subset=['date']).copy()\n",
    "        edges_with_dates['period'] = edges_with_dates['date'].dt.to_period(TIME_SLICE_FREQ).dt.to_timestamp()\n",
    "        for period, chunk in edges_with_dates.groupby('period'):\n",
    "            agg_chunk = aggregate_edges(chunk)\n",
    "            if MIN_EDGE_WEIGHT and MIN_EDGE_WEIGHT > 1:\n",
    "                agg_chunk = agg_chunk[agg_chunk['weight'] >= MIN_EDGE_WEIGHT]\n",
    "            if KEEP_GIANT_COMPONENT:\n",
    "                agg_chunk = keep_giant_component(agg_chunk)\n",
    "            chunk_metrics = None\n",
    "            if AUTO_PRUNE_BY_DEGREE:\n",
    "                agg_chunk, chunk_metrics, degree_threshold, kept_nodes = prune_by_degree(\n",
    "                    agg_chunk,\n",
    "                    min_degree=PRUNE_DEGREE_MIN,\n",
    "                    quantile=PRUNE_DEGREE_QUANTILE,\n",
    "                    top_n=PRUNE_DEGREE_TOP_N,\n",
    "                )\n",
    "            elif TOP_N_NODES:\n",
    "                agg_chunk = keep_top_n_nodes(agg_chunk, TOP_N_NODES)\n",
    "            gexf_path = Path(OUTPUT_DIR) / f'enron_network_{period.strftime(\"%Y-%m-%d\")}.gexf'\n",
    "            export_gexf(agg_chunk, gexf_path, node_metrics=chunk_metrics)\n",
    "        print(f'Time-sliced GEXF files written to {OUTPUT_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577251a",
   "metadata": {},
   "source": [
    "**Next Steps:** Update `INPUT_MODE`, `INPUT_PATH`, and pruning values to match your local dataset. After running the notebook, open the generated `.gexf` files in Gephi to explore degree-filtered communication networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
